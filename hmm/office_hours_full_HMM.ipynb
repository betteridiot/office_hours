{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np            # Workhorse\n",
    "import numpy.random as nr     # Setting up random distributions\n",
    "import pandas as pd           # Simple convergence checks\n",
    "from itertools import product # For robust iteration\n",
    "from copy import deepcopy     # For convergence checking\n",
    "from array import array       # Faster base Python iteration\n",
    "\n",
    "try:\n",
    "    # Special Json library that isn't only more efficient, \n",
    "    # but also allows for setting floating point precision on print\n",
    "    import ujson as json\n",
    "\n",
    "    def to_json(data, precision = 2):\n",
    "        \"\"\"Wrapper function to output dict to json with specific floating-point\n",
    "        precision.\n",
    "\n",
    "        Args:\n",
    "            data (dict): some HMM probability matrix\n",
    "            precision (int): Setting for floating-point precision (default: 2)\n",
    "        \n",
    "        Returns:\n",
    "            (json): JSON representation of the dictionary\n",
    "        \"\"\"\n",
    "        return json.dumps(data, indent = 4, double_precision = precision)\n",
    "\n",
    "except ImportError:\n",
    "    import json\n",
    "\n",
    "    def to_json(data, precision = 2):\n",
    "        \"\"\"Wrapper function to output dict to json with specific floating-point\n",
    "        precision.\n",
    "\n",
    "        Note: Since the base `json.dumps` doesn't provide double_precision,\n",
    "        this goes through an extra encoder step to handle the floats\n",
    "\n",
    "        Args:\n",
    "            data (dict): some HMM probability matrix\n",
    "            precision (int): Setting for floating-point precision (default: 2)\n",
    "\n",
    "        Returns:\n",
    "            (json): JSON representation of the dictionary\n",
    "        \"\"\"\n",
    "        out_json = json.dumps(data)\n",
    "        loaded_json = json.loads(out_json, parse_float=lambda o: f'{float(o):.2g}')\n",
    "        return json.dumps(loaded_json, sort_keys = True, indent = 4)\n",
    "\n",
    "# Gil-ify\n",
    "np.set_printoptions(precision = 2)\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseHMM(object):\n",
    "    \"\"\"Base class for HMM objects\n",
    "\n",
    "    Class for holding HMM parameters and to allow for implementation of\n",
    "    functions associated with HMMs\n",
    "\n",
    "    Attributes:\n",
    "        alphabet (str): The emissions used in the HMM (default: 'ACGT')\n",
    "        hidden_states (str): The hidden states within the HMM (default: None)\n",
    "        init_probs (dict of floats): β probabilities for initial steps (default: None)\n",
    "        trans_probs (dict of dict of floats): Transition probabilities from one state to another given a state (default: None)\n",
    "        emit_probs (dict of dict of floats): Emission probabilities of a letter given a state (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    __all__ = ['alphabet', 'hidden_states', 'emit_probs', 'trans_probs', 'init_probs']\n",
    "\n",
    "    def __init__(self, alphabet = \"ACGT\", hidden_states = None, init_probs = None, \n",
    "                 trans_probs = None, emit_probs = None, seed = None, precision = 2, tolerance = 1e-10):\n",
    "        \"\"\"Instantiate the HMM\n",
    "\n",
    "        Args:\n",
    "            alphabet (str): The emissions used in the HMM (default: 'ACGT')\n",
    "            hidden_states (str): The hidden states within the HMM (default: None)\n",
    "            init_probs (dict of floats): β probabilities for initial steps (default: None)\n",
    "            trans_probs (dict of dict of floats): Transition probabilities from one state to another given a state (default: None)\n",
    "            emit_probs (dict of dict of floats): Emission probabilities of a letter given a state (default: None)\n",
    "            seed (int): To set the random seed for numpy (default: None)\n",
    "            precision (int): Floating-point precision (default: 2)\n",
    "            tolerance (number): The acceptable tolerance between baum-welch iteration to determine if convergence has occured (default: 1e-10)\n",
    "        \"\"\"\n",
    "\n",
    "        # Floating point precision\n",
    "        self._precision = precision\n",
    "\n",
    "        # For convergence checking\n",
    "        self._tolerance = tolerance\n",
    "\n",
    "        # Need at least the hidden states to initialize\n",
    "        if hidden_states is None:\n",
    "            raise ValueError('Hidden states must be provided')\n",
    "        self.hidden_states = hidden_states\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "        # Cursory initialization that allows users to set some prior\n",
    "        self.init_probs = init_probs\n",
    "        self.trans_probs = trans_probs\n",
    "        self.emit_probs = emit_probs\n",
    "        \n",
    "        # If the user doesn't provide any probabilities, randomize some stuff\n",
    "        self._initialize_random(seed)\n",
    "    \n",
    "    def _initialize_random(self, seed):\n",
    "        \"\"\"[PRIVATE] Used to completely initialize the HMM if any probability matrices\n",
    "        provided. Will not overwrite given probabilities.\n",
    "\n",
    "        Args:\n",
    "            seed (int): To set the random seed for numpy\n",
    "        \"\"\"\n",
    "        nr.seed(seed)\n",
    "        if self.init_probs is None:\n",
    "\n",
    "            # These aliases help keep line width down during the iteration and allows \n",
    "            # for simple dictionary comprehension\n",
    "            states = self.hidden_states\n",
    "            i_probs = nr.dirichlet(np.ones(len(self.hidden_states)))\n",
    "\n",
    "            # Make a dictionary of floats (that sum up to 1) for all states in the HMM\n",
    "            init = {state: np.float64(i_prob) for state, i_prob in zip(states, i_probs)}\n",
    "            self.init_probs = init\n",
    "\n",
    "        if self.trans_probs is None:\n",
    "\n",
    "            # These aliases help keep line width down during the iteration and allows \n",
    "            # for simple dictionary comprehension\n",
    "            states = product(self.hidden_states, self.hidden_states)\n",
    "            t_probs = np.nditer(nr.dirichlet(np.ones(len(self.hidden_states)), size=len(self.hidden_states)))\n",
    "\n",
    "            # Make a dictionary for each state that contains a dictionary of floats\n",
    "            # that sum to 1 and represent each of the states used in the HMM\n",
    "            trans = {}\n",
    "            for (state, next_state), t_prob in zip(states, t_probs):\n",
    "                trans.setdefault(state, {}).update({next_state: np.float64(t_prob)})\n",
    "            self.trans_probs = trans\n",
    "\n",
    "        if self.emit_probs is None:\n",
    "\n",
    "            # These aliases help keep line width down during the iteration and allows \n",
    "            # for simple dictionary comprehension\n",
    "            states_letters = product(self.hidden_states, self.alphabet)\n",
    "            e_probs = np.nditer(nr.dirichlet(np.ones(len(self.alphabet)), size=len(self.hidden_states)))\n",
    "\n",
    "            # Make a dictionary for each state that contains a dictionary of floats\n",
    "            # that sum to 1 and represent each of the emissions used in the HMM\n",
    "            emit = {}\n",
    "            for (state, letter), e_prob in zip(states_letters, e_probs):\n",
    "                emit.setdefault(state, {}).update({letter: np.float64(e_prob)})\n",
    "            self.emit_probs = emit\n",
    "\n",
    "    # All the `@property` and `@''.setter` methods are doing are allowing\n",
    "    # for isolation and encapsulation of the internal datasets\n",
    "\n",
    "    @property\n",
    "    def init_probs(self):\n",
    "        return self._initial\n",
    "\n",
    "    @init_probs.setter\n",
    "    def init_probs(self, init_probs):\n",
    "        if init_probs is None:\n",
    "            self._initial = None\n",
    "        elif isinstance(init_probs, dict):\n",
    "            if len(init_probs) != len(self.hidden_states):\n",
    "                raise ValueError('Initial probabilites must be the length of the number of hidden states')\n",
    "            if not np.isclose(sum(init_probs.values()), 1):\n",
    "                raise ValueError('Initial probabilites must sum to 1')\n",
    "            self._initial = init_probs\n",
    "        else:\n",
    "            raise SyntaxError('Initial probabilities must be None or a dictionary')\n",
    "\n",
    "    @property\n",
    "    def trans_probs(self):\n",
    "        return self._transition\n",
    "\n",
    "    @trans_probs.setter\n",
    "    def trans_probs(self, trans_probs):\n",
    "        if trans_probs is None:\n",
    "            self._transition = None\n",
    "        elif isinstance(trans_probs, dict):\n",
    "            if len(trans_probs) != len(self.hidden_states):\n",
    "                raise ValueError('Transition probabilites must be a square matrix')\n",
    "            if len(trans_probs[self.hidden_states[0]]) != len(self.hidden_states):\n",
    "                raise ValueError('Transition probabilites must be a square matrix')\n",
    "            if not np.allclose([sum(trans_probs[state].values()) for state in self.hidden_states], 1):\n",
    "                raise ValueError('Transition probabilites must sum to 1 along a given axis')\n",
    "            self._transition = trans_probs\n",
    "        else:\n",
    "            raise SyntaxError('Transition probabilities must be None or a dictionary')\n",
    "\n",
    "    @property\n",
    "    def emit_probs(self):\n",
    "        return self._emission\n",
    "\n",
    "    @emit_probs.setter\n",
    "    def emit_probs(self, emit_probs):\n",
    "        if emit_probs is None:\n",
    "            self._emission = None\n",
    "        elif isinstance(emit_probs, dict):\n",
    "            if len(emit_probs) != len(self.hidden_states):\n",
    "                raise ValueError('Emission probabilites must be length of hidden states by length of alphabet')\n",
    "            if len(emit_probs[self.hidden_states[0]]) != len(self.alphabet):\n",
    "                raise ValueError('Emission probabilites must be length of hidden states by length of alphabet')\n",
    "            emit = pd.DataFrame.from_dict(emit_probs).T\n",
    "            emit.columns = list(self.alphabet)\n",
    "            if not np.allclose([sum(emit_probs[state].values()) for state in self.hidden_states], 1):\n",
    "                raise ValueError('Emission probabilites must sum to 1 along a given axis')\n",
    "            self._emission = emit_probs\n",
    "        else:\n",
    "            raise SyntaxError('Emission probabilities must be None or a dictionary')\n",
    "\n",
    "    @property\n",
    "    def hidden_states(self):\n",
    "        return self._hidden_states\n",
    "\n",
    "    @hidden_states.setter\n",
    "    def hidden_states(self, hidden_states):\n",
    "        if isinstance(hidden_states, str):\n",
    "            self._hidden_states = hidden_states\n",
    "        elif isinstance(hidden_states, (tuple, list)):\n",
    "            self._hidden_states = ''.join(hidden_states)\n",
    "\n",
    "    @property\n",
    "    def alphabet(self):\n",
    "        return self._alph\n",
    "\n",
    "    @alphabet.setter\n",
    "    def alphabet(self, alphabet):\n",
    "        if isinstance(alphabet, str):\n",
    "            self._alph = alphabet\n",
    "        elif isinstance(alphabet, (tuple, list)):\n",
    "            self._alph = ''.join(alphabet)\n",
    "\n",
    "    def __str__(self):\n",
    "        out_text = [f'Alphabet: {self.alphabet}',\n",
    "                    f'Hidden States: {self.hidden_states}',\n",
    "                    f'Initial Probabilities: {to_json(self.init_probs, self._precision)}',\n",
    "                    f'Transition Probabilities: {to_json(self.trans_probs, self._precision)}',\n",
    "                    f'Emission Probabilities: {to_json(self.emit_probs, self._precision)}']\n",
    "        return '\\n'.join(out_text)\n",
    "\n",
    "    @classmethod\n",
    "    def __dir__(cls):\n",
    "        return cls.__all__\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Solely used by the `baum_welch` function to check for convergence\"\"\"\n",
    "        if np.allclose(pd.Series(self.init_probs), pd.Series(other.init_probs), atol=self._tolerance):\n",
    "            if np.allclose(pd.DataFrame(self.trans_probs), pd.DataFrame(other.trans_probs), atol=self._tolerance):\n",
    "                if np.allclose(pd.DataFrame(self.emit_probs), pd.DataFrame(other.emit_probs), atol=self._tolerance):\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(BaseHMM):\n",
    "    \"\"\"Main class for HMM objects\n",
    "\n",
    "    Class for holding HMM parameters and to allow for implementation of\n",
    "    functions associated with HMMs\n",
    "\n",
    "    Attributes:\n",
    "        alphabet (str): The emissions used in the HMM (default: 'ACGT')\n",
    "        hidden_states (str): The hidden states within the HMM (default: None)\n",
    "        init_probs (dict of floats): β probabilities for initial steps (default: None)\n",
    "        trans_probs (dict of dict of floats): Transition probabilities from one state to another given a state (default: None)\n",
    "        emit_probs (dict of dict of floats): Emission probabilities of a letter given a state (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    __all__ = ['alphabet', 'hidden_states', 'emit_probs', 'trans_probs', 'init_probs',\n",
    "               'viterbi','forward', 'backward', 'forward_backward', 'baum-welch']\n",
    "\n",
    "    @classmethod\n",
    "    def __dir__(cls):\n",
    "        return cls.__all__\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        \"\"\" The forward algorithm for calculating probability of sequence given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (str): a valid set of emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            forward_prob (float): probability of the the sequence using the forward algorithm\n",
    "            forward (dict of array of float): the forward matrix of the probabilities of given bases at given positions and given states\n",
    "        \"\"\"\n",
    "        # Initialize the lattice. I am using an array here because it is more\n",
    "        # performant in iteration and has a smaller footprint than mutliple dictionaries\n",
    "        forward = {state: array('d', [0] * len(sequence)) for state in self.hidden_states}\n",
    "\n",
    "        # Since the forward algorithm starts at the beginning, use the first emission\n",
    "        # and initial state probabilities to fill in the first column\n",
    "        for state in self.hidden_states:\n",
    "            forward[state][0] = self.init_probs[state] * self.emit_probs[state][sequence[0]]\n",
    "\n",
    "        # This is where things get interesting: by taking the cartesian product of the\n",
    "        # index position (along the sequence, starting from 1 since 0 is already filled in) and\n",
    "        # The hidden states, we can condense a nested for-loop into a single line.\n",
    "        for seq_idx, next_state in product(range(1, len(sequence)), self.hidden_states):\n",
    "\n",
    "            # Since the forward algorithm just takes the sum across states from a given state, we need \n",
    "            # to tease each of the states out.\n",
    "            # Furthermore, this is always using the data that has already been calculated from the left.\n",
    "            for curr_state in self.hidden_states:\n",
    "                forward[next_state][seq_idx] += forward[curr_state][seq_idx - 1] * self.trans_probs[curr_state][next_state]\n",
    "\n",
    "            # Now that we have our positional sum, we use the emission probability for that state to update\n",
    "            forward[next_state][seq_idx] = (self.emit_probs[next_state][sequence[seq_idx]] \n",
    "                                          * forward[next_state][seq_idx])\n",
    "\n",
    "        # When all is done, the final probability of the sequence (based on the forward algorithm),\n",
    "        # is the sum across both states at the end\n",
    "        forward_prob = sum(forward[state][-1] for state in self.hidden_states)\n",
    "        return forward_prob, forward \n",
    "\n",
    "    def backward(self, sequence):\n",
    "        \"\"\" The backward algorithm for calculating probability of sequence given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (str): a valid set of emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            backward_prob (float): probability of the the sequence using the backward algorithm\n",
    "            backward (dict of array of float): the backward matrix of the probabilities of given bases at given positions and given states\n",
    "        \"\"\"\n",
    "        # Initialize the lattice. I am using an array here because it is more\n",
    "        # performant in iteration and has a smaller footprint than mutliple dictionaries\n",
    "        backward = {state: array('d', [0] * len(sequence)) for state in self.hidden_states}\n",
    "\n",
    "        # The backward algorithm starts at the end. Make that 1 and work from there\n",
    "        for state in self.hidden_states:\n",
    "            backward[state][-1] = 1\n",
    "\n",
    "        # Like the forward algorithm: by taking the cartesian product of the\n",
    "        # index position (along the sequence starting from the end since) and\n",
    "        # The hidden states, we can condense a nested for-loop into a single line.\n",
    "        rev_seq = range(len(sequence) - 1, 0, -1)\n",
    "        for seq_idx, last_state in product(rev_seq, self.hidden_states):\n",
    "\n",
    "            # Since the forward algorithm just takes the sum across states from a given state, we need \n",
    "            # to tease each of the states out\n",
    "            # Furthermore, this is always using the data that has already been calculated from the right.\n",
    "            for curr_state in self.hidden_states:\n",
    "                backward[last_state][seq_idx-1] += backward[curr_state][seq_idx] * self.trans_probs[last_state][curr_state] * self.emit_probs[curr_state][sequence[seq_idx]]\n",
    "\n",
    "        # When all is done, the final probability of the sequence (based on the backward algorithm),\n",
    "        # is the sum across both states at the start (relative to left-to-right)\n",
    "        backward_prob = sum(backward[state][0]*self.init_probs[state] * self.emit_probs[state][sequence[0]] for state in self.hidden_states)\n",
    "        return backward_prob, backward\n",
    "\n",
    "    def forward_backward(self, sequence):\n",
    "        \"\"\" The forward-backward algorithm for calculating marginal posteriors given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            posterior (list of dicts): all posteriors as a list\n",
    "        \"\"\"\n",
    "        #Calculate forward and backward matrices\n",
    "        Pf, f_matrix = self.forward(sequence)\n",
    "        Pb, b_matrix = self.backward(sequence)\n",
    "\n",
    "        # Generally speaking, most will only use either the forward or the backward\n",
    "        # probability of the sequence...not both. However, these two probabilities\n",
    "        # should be nearly the identical and I like being conservative. So I use the\n",
    "        # average and everybody is included.\n",
    "        P = (Pf + Pb)/2\n",
    "\n",
    "        # Initialize the lattice. I am using an array here because it is more\n",
    "        # performant in iteration and has a smaller footprint than mutliple dictionaries\n",
    "        posterior = {state: array('d', [0] * len(sequence)) for state in self.hidden_states}\n",
    "\n",
    "        # By using the cartesian product of the range of the sequence length and\n",
    "        # the hidden states, I can condense a nested for-loop to a singe line\n",
    "        for i, state in product(range(len(sequence)), self.hidden_states):\n",
    "            posterior[state][i] = f_matrix[state][i] * b_matrix[state][i] / P  \n",
    "        return posterior\n",
    "\n",
    "    def viterbi(self, sequence):\n",
    "        \"\"\" The viterbi algorithm for decoding a string using a HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (str): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (str): optimal path through HMM given the model parameters\n",
    "                           using the Viterbi algorithm\n",
    "        \"\"\"\n",
    "        def update_probs(base, previous):\n",
    "            \"\"\"Nested function used to keep track of the current probabilities and update the next\n",
    "\n",
    "            Args:\n",
    "                base (str): the current emission\n",
    "                previous (dict of float): previous position's probabilities\n",
    "\n",
    "            Returns:\n",
    "                next_prob (dict of float): Next position's probabilities\n",
    "                tb (dict of str): The traceback from current to previous origin\n",
    "            \"\"\"\n",
    "            curr_prob = {} # Will caclculate our current position's probabilities\n",
    "            next_prob = {} # Will contain the new position's probabilities\n",
    "            tb = {}        # Contains the computed traceback as {current_state: previous} entries\n",
    "\n",
    "            for next_state in self.hidden_states:\n",
    "                for curr_state in self.hidden_states:\n",
    "                    curr_prob[curr_state] = previous[curr_state] + np.log10(self.trans_probs[curr_state][next_state])\n",
    "\n",
    "                # This max function acts as a argmax. This is because the key parameter can take a function\n",
    "                # to determine how max is computed. Here we are telling it to base it on the values of the keys\n",
    "                # and not the keys themselves and then return the key matching the max value\n",
    "                origin = max(curr_prob, key=curr_prob.get)\n",
    "\n",
    "                # We use that origin the next states probability based on the current emission\n",
    "                next_prob[next_state] = np.log10(self.emit_probs[next_state][base]) + curr_prob[origin]\n",
    "                tb[next_state] = origin\n",
    "            return next_prob, tb\n",
    "\n",
    "        def get_traceback(traceback, last_origin):\n",
    "            \"\"\"Nested function that parses the traceback dict and constructs the most optimal \n",
    "            path of states given the sequence\n",
    "\n",
    "            Args:\n",
    "                traceback (dict of str): The traceback of all positions to their origins\n",
    "                last_origin (str): the max state from last position of the probability matrix\n",
    "\n",
    "            Returns:\n",
    "                tb (str):  the rest of the path, starting from last_origin\n",
    "            \"\"\"\n",
    "            tb = ''\n",
    "\n",
    "            # Reverse the traceback so that we start at the end\n",
    "            for pos in reversed(traceback):\n",
    "                # We already determine last_origin based on the final outcome\n",
    "                # of the probability matrix\n",
    "                prev_origin = pos[last_origin]\n",
    "\n",
    "                # Keep adding to our sequence of optimal origins\n",
    "                tb += prev_origin\n",
    "\n",
    "                # Update for the next iteration\n",
    "                last_origin = prev_origin\n",
    "            return tb\n",
    "\n",
    "        traceback = []\n",
    "\n",
    "        first_base = sequence[0]\n",
    "\n",
    "        # Start off by using the initial conditions and the first emission of the sequence\n",
    "        previous = {state: np.log10(self.init_probs[state]) + np.log10(self.emit_probs[state][first_base]) for state in self.hidden_states}\n",
    "\n",
    "        # Go through all other positions and keep track of the running total of probabilities\n",
    "        for base in sequence[1:]:\n",
    "            update_previous, update_tb = update_probs(base, previous)\n",
    "            previous = update_previous\n",
    "            traceback.append(update_tb)\n",
    "\n",
    "        # Find the max state at the final position\n",
    "        result = max(previous, key=previous.get)\n",
    "\n",
    "        result += get_traceback(traceback, result)        \n",
    "\n",
    "        # Since Traceback starts at the end and works forward, \n",
    "        # we need to reverse the result\n",
    "        return result[::-1]\n",
    "\n",
    "    def baum_welch(self, sequences, pseudocount = 1e-100):\n",
    "        \"\"\"Baum-Welch is an EM-algorithm that finds the maximum likelihood estimate of \n",
    "        the parameters of a HMM given a set of observed emission sequences.\n",
    "\n",
    "        Note: Used when the user doesn't know all/any of the HMM's probabilities.\n",
    "\n",
    "        Args:\n",
    "            sequences (list of str): all the sequences used for training the HMM\n",
    "            pseudocount (number): some pseudocount to prevent ZeroDivisionError (default: 1e-100)\n",
    "        \"\"\"\n",
    "        def init_bw(pseudocount):\n",
    "            \"\"\"Initializes the pseudocount-filled probability matrices for init, trans, and emit\n",
    "\n",
    "            Args:\n",
    "                pseudocount (number): some pseudocount to prevent ZeroDivisionError\n",
    "\n",
    "            Returns:\n",
    "                init (dict of floats): Pseudocount-filled matrix for initial steps\n",
    "                trans (dict of dict of floats): Pseudocount-filled matrix for transition probabilities from one state to another given a state\n",
    "                emit (dict of dict of floats): Pseudocount-filled matrix for emission probabilities of a letter given a state\n",
    "            \"\"\"\n",
    "            init = {state: pseudocount for state in self.hidden_states}\n",
    "            trans = {state: {next_state: pseudocount for next_state in self.hidden_states} for state in self.hidden_states}\n",
    "            emit = {state: {letter: pseudocount for letter in self.alphabet} for state in self.hidden_states}\n",
    "            return init, trans, emit\n",
    "        \n",
    "        def proc_seq(outer, seq, init, trans, emit):\n",
    "            \"\"\"Processes a given sequence such that init, trans, and emit \n",
    "            are updated as specific emission are computed.\n",
    "\n",
    "            Args:\n",
    "                outer (float): the sum of observed sequence probabilities\n",
    "                init (dict of floats): β probabilities for initial steps\n",
    "                trans (dict of dict of floats): Transition probabilities from one state to another given a state\n",
    "                emit (dict of dict of floats): Emission probabilities of a letter given a state\n",
    "\n",
    "            Returns:\n",
    "                outer (float): incremented sum of observed sequence probabilities\n",
    "                init (dict of floats): Scaled β probabilities for initial steps\n",
    "                trans (dict of dict of floats): Scaled Transition probabilities from one state to another given a state\n",
    "                emit (dict of dict of floats): Scaled Emission probabilities of a letter given a state\n",
    "            \"\"\"\n",
    "            prob_forward, forward = self.forward(seq)\n",
    "            prob_backward, backward = self.backward(seq)\n",
    "\n",
    "            # Fun Durbin step because, ummm...stats?\n",
    "            # Generally speaking, most will only use either the forward or the backward\n",
    "            # probability of the sequence...not both. However, these two probabilities\n",
    "            # should be nearly the identical and I like being conservative. So I use the\n",
    "            # average and everybody is included.\n",
    "            outer += (prob_forward + prob_backward)/2\n",
    "\n",
    "            # As I go through the sequence one emission at a time...\n",
    "            for i, emission in enumerate(seq):\n",
    "                # and visit each possible state that emission can be from...\n",
    "                for state in self.hidden_states:\n",
    "                    # I need to take into account the very first observation coming from the initial step\n",
    "                    if i == 0:\n",
    "                        init[state] += forward[state][i] * backward[state][i]\n",
    "                    # and update the emission matrix for every emission observed at the given step\n",
    "                    emit[state][emission] += forward[state][i] * backward[state][i]\n",
    "\n",
    "                    if i == len(seq) - 1:\n",
    "                        break # I have reached the end of the sequence wrt to transitions, so I stop\n",
    "\n",
    "                    # Transitions are fun because it is always based on where it can go from where it is.\n",
    "                    # Therefore we are always looking ahead\n",
    "                    trans_emission = seq[i+1]\n",
    "                    for next_state in self.hidden_states:\n",
    "                        # Based on the 'future' state given our 'current' state, we use the probability of\n",
    "                        # the next emission at the next position at the next state to update our transition\n",
    "                        # matrix\n",
    "                        trans[state][next_state] += (\n",
    "                            forward[state][i] \n",
    "                            * self.trans_probs[state][next_state]\n",
    "                            * self.emit_probs[next_state][trans_emission] \n",
    "                            * backward[next_state][i+1]\n",
    "                        )\n",
    "\n",
    "            return outer, init, trans, emit\n",
    "\n",
    "        def scale_step(outer, init, trans, emit):\n",
    "            \"\"\"Scales all the probability matrices based on the sum of\n",
    "            observed sequence probabilities.\n",
    "\n",
    "            Args:\n",
    "                outer (float): the sum of observed sequence probabilities\n",
    "                init (dict of floats): β probabilities for initial steps\n",
    "                trans (dict of dict of floats): Transition probabilities from one state to another given a state\n",
    "                emit (dict of dict of floats): Emission probabilities of a letter given a state\n",
    "\n",
    "            Returns:\n",
    "                init (dict of floats): Scaled β probabilities for initial steps\n",
    "                trans (dict of dict of floats): Scaled Transition probabilities from one state to another given a state\n",
    "                emit (dict of dict of floats): Scaled Emission probabilities of a letter given a state\n",
    "            \"\"\"\n",
    "            # Use that fun outer denominator we have been keeping track of. However,\n",
    "            # if only one sequence is provided, the user could just use the posterior\n",
    "            # (or forward_backward algorithm) instead.\n",
    "            # For each of the matrices, we are essentially dividing all of our observed\n",
    "            # probabilities by the probability of the sequences. If we don't we will greatly\n",
    "            # underestimate our probabilities and likely hit an underflow issue\n",
    "            for state in self.hidden_states:\n",
    "                init[state] /= outer\n",
    "                for letter in self.alphabet:\n",
    "                    emit[state][letter] /= outer\n",
    "                for other_state in self.hidden_states:\n",
    "                    trans[state][other_state] /= outer\n",
    "\n",
    "            # I think this is the Maximization step. However, since all of our probabilities\n",
    "            # should just sum to 1 for any given state, this shouldn't have a large impact\n",
    "            init_sum = sum(init.values())\n",
    "            emit_sum = {state: sum(emit[state].values()) for state in self.hidden_states}\n",
    "            trans_sum = {state: sum(trans[state].values()) for state in self.hidden_states}            \n",
    "\n",
    "            for state in self.hidden_states:\n",
    "                init[state] /= init_sum\n",
    "                for letter in self.alphabet:\n",
    "                    emit[state][letter] /= emit_sum[state]\n",
    "                for other_state in self.hidden_states:\n",
    "                    trans[state][other_state] /= trans_sum[state]\n",
    "            return init, trans, emit\n",
    "\n",
    "        converged = False\n",
    "        count = 0\n",
    "\n",
    "        while not converged:\n",
    "            init, trans, emit = init_bw(pseudocount)\n",
    "\n",
    "            # This is only used if there are multiple sequences\n",
    "            # otherwise, forward-backward would be okay\n",
    "            outer = 0 \n",
    "\n",
    "            for seq in sequences:\n",
    "\n",
    "                # Send each sequence to processing\n",
    "                outer, init, trans, emit = proc_seq(outer, seq, init, trans, emit)\n",
    "\n",
    "            # Now scale the probability matrices based on sum of observed sequence probabilities\n",
    "            init, trans, emit = scale_step(outer, init, trans, emit)\n",
    "\n",
    "            # Used for convergence checking\n",
    "            old = deepcopy(self)\n",
    "\n",
    "            # Update the model\n",
    "            self.init_probs = init\n",
    "            self.emit_probs = emit\n",
    "            self.trans_probs = trans\n",
    "\n",
    "            count += 1\n",
    "            # Utilize that special HMM.__eq__ method\n",
    "            if self == old:\n",
    "                converged = True\n",
    "                print(f'Converged after {count} iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HMM(hidden_states='GI', precision=2, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet: ACGT\n",
      "Hidden States: GI\n",
      "Initial Probabilities: {\n",
      "    \"G\":0.13,\n",
      "    \"I\":0.87\n",
      "}\n",
      "Transition Probabilities: {\n",
      "    \"G\":{\n",
      "        \"G\":0.59,\n",
      "        \"I\":0.41\n",
      "    },\n",
      "    \"I\":{\n",
      "        \"G\":0.5,\n",
      "        \"I\":0.5\n",
      "    }\n",
      "}\n",
      "Emission Probabilities: {\n",
      "    \"G\":{\n",
      "        \"A\":0.01,\n",
      "        \"C\":0.48,\n",
      "        \"G\":0.22,\n",
      "        \"T\":0.29\n",
      "    },\n",
      "    \"I\":{\n",
      "        \"A\":0.0,\n",
      "        \"C\":0.63,\n",
      "        \"G\":0.32,\n",
      "        \"T\":0.04\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 52 iterations\n"
     ]
    }
   ],
   "source": [
    "model.baum_welch([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet: ACGT\n",
      "Hidden States: GI\n",
      "Initial Probabilities: {\n",
      "    \"G\":1.0,\n",
      "    \"I\":0.0\n",
      "}\n",
      "Transition Probabilities: {\n",
      "    \"G\":{\n",
      "        \"G\":0.92,\n",
      "        \"I\":0.08\n",
      "    },\n",
      "    \"I\":{\n",
      "        \"G\":0.15,\n",
      "        \"I\":0.85\n",
      "    }\n",
      "}\n",
      "Emission Probabilities: {\n",
      "    \"G\":{\n",
      "        \"A\":0.48,\n",
      "        \"C\":0.07,\n",
      "        \"G\":0.08,\n",
      "        \"T\":0.37\n",
      "    },\n",
      "    \"I\":{\n",
      "        \"A\":0.0,\n",
      "        \"C\":0.69,\n",
      "        \"G\":0.31,\n",
      "        \"T\":0.0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\n",
      "GIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIGGGGG\n"
     ]
    }
   ],
   "source": [
    "print(sequence)\n",
    "print(model.viterbi(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
