{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# betteridiot Office Hours\n",
    "## Today's Topic: Hidden Markov Models (HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/HMMGraph.svg/2000px-HMMGraph.svg.png)\n",
    "Taken from https://en.wikipedia.org/wiki/Hidden_Markov_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Examples from the [Class GitHub](https://github.com/dcmb-courses/bioinf529-winter2019/blob/master/classes/class_7/class7.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The letters used for our sequences\n",
    "alphabet = 'ACGT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unseen states inferred by observations\n",
    "hidden_states = 'IG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where do we start\n",
    "initial_probabilities = {\n",
    "    'I' : 0.1,\n",
    "    'G' : 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the conditions in which we change\n",
    "transition_probabilities = {\n",
    "    'I': { 'I' : 0.6, 'G' : 0.4 },\n",
    "    'G': { 'I' : 0.1, 'G' : 0.9 }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we do when we get there\n",
    "emission_probabilities = {\n",
    "    'I': { 'A' : 0.1, 'C' : 0.4, 'G' : 0.4, 'T' : 0.1 },\n",
    "    'G': { 'A' : 0.4, 'C' : 0.1, 'G' : 0.1, 'T' : 0.4 }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the data structure to handle this all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \"\"\"Main class for HMM objects\n",
    "\n",
    "    Atttibutes:\n",
    "        alphabet (set): The alphabet of emissions\n",
    "        hidden_states (set): Hidden states in the model\n",
    "        initial_probs (dict): A dictionary of initial state probabilities (default: None)\n",
    "        trans_probs (dict of dict): A dictionary of transition probabilities (default: None)\n",
    "        emit_probs (dict of dict): A dictionary of emission probabilities (default: None)\n",
    "    \"\"\"\n",
    "    \n",
    "    __all__ = ['alphabet', 'hidden_states', 'trans_probs', 'initial_probs', 'emit_probs', 'viterbi']\n",
    "    \n",
    "    def __init__(self, alphabet, hidden_states, β = None, trans_probs = None, emit_probs = None):\n",
    "        \"\"\"Instaniates the object\n",
    "\n",
    "        Args:\n",
    "            alphabet (str): The alphabet of emissions\n",
    "            hidden_states (list of str): Hidden states in the model\n",
    "            β (dict of float): A dictionary of initial state probabilities (default: None)\n",
    "            trans_probs (dict of dict): A dictionary of transition probabilities (default: None)\n",
    "            emit_probs (dict of dict): A dictionary of emission probabilities (default: None)\n",
    "        \"\"\"\n",
    "        self.alphabet = set(alphabet)\n",
    "        self.hidden_states = set(hidden_states)\n",
    "        self._β = β\n",
    "        self.initial_probs = {key: np.log10(val) for key, val in β.items()}\n",
    "        self._t = trans_probs\n",
    "        self.trans_probs = self._transform_dict(trans_probs)\n",
    "        self._e = emit_probs\n",
    "        self.emit_probs = self._transform_dict(emit_probs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _transform_dict(nested_dict):\n",
    "        \"\"\"Transforms a dict of dict of floating point probabilites to log10 equivalent\n",
    "\n",
    "        Args:\n",
    "            nested_dict (dict of dict of floats): dictionary of probabilities wrt hidden state\n",
    "\n",
    "        Returns:\n",
    "            out_dict (dict of dict of floats): log10 transformed probabilities\n",
    "        \"\"\"\n",
    "        out_dict = {}\n",
    "        for key_outer, sub_dict in nested_dict.items():\n",
    "            for key_inner, val in sub_dict.items():\n",
    "                out_dict.setdefault(key_outer, {}).update({key_inner: np.log10(val)})\n",
    "        return out_dict\n",
    "    \n",
    "    def __str__(self):\n",
    "        out_text = [f'Alphabet: {self.alphabet}',\n",
    "        f'Hidden States: {self.hidden_states}',\n",
    "        f'Initial Probabilities: {json.dumps(self._β, sort_keys = True, indent = 4)}',\n",
    "        f'Transition Probabilities: {json.dumps(self._t, sort_keys = True, indent = 4)}',\n",
    "        f'Emission Probabilities: {json.dumps(self._e, sort_keys = True, indent = 4)}']\n",
    "        return '\\n'.join(out_text)\n",
    "    \n",
    "    @classmethod\n",
    "    def __dir__(cls):\n",
    "        return cls.__all__\n",
    "    \n",
    "    def viterbi(self, sequence):\n",
    "        \"\"\" The Viterbi algorithm for decoding a string using a HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (str): Sequence of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (str): optimal path through HMM given the model parameters\n",
    "                           using the Viterbi algorithm\n",
    "        \"\"\"\n",
    "        traceback = []\n",
    "\n",
    "        first_base = sequence[0]\n",
    "\n",
    "        previous = {state: self.initial_probs[state] + self.emit_probs[state][first_base] for state in self.hidden_states}\n",
    "#         previous = {}\n",
    "#         for state in self.hidden_states:\n",
    "#             previous.update({state: self.initial_probs[state] + self.emit_probs[state][first_base]})\n",
    "\n",
    "        for base in sequence[1:]:\n",
    "            update_previous, update_tb = self.update_probs(base, previous)\n",
    "\n",
    "            previous = update_previous\n",
    "            traceback.append(update_tb)\n",
    "\n",
    "        result = max(previous, key = previous.get)\n",
    "\n",
    "        result += self.get_traceback(traceback, result)\n",
    "        return result[::-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_traceback(traceback, last_origin):\n",
    "        tb = ''\n",
    "        for pos in reversed(traceback):\n",
    "            prev_origin = pos[last_origin]\n",
    "            tb += prev_origin\n",
    "            last_origin = prev_origin\n",
    "        return tb\n",
    "\n",
    "    def update_probs(self, base, previous):\n",
    "        curr_prob = {}\n",
    "        tb_pos = {}\n",
    "\n",
    "        for future in self.hidden_states:\n",
    "            check = {current: previous[current] + self.trans_probs[current][future] for current in self.hidden_states}\n",
    "            origin = max(check, key = check.get)\n",
    "            curr_prob.update({future: self.emit_probs[future][base] + check[origin]})\n",
    "            tb_pos.update({future: origin})\n",
    "\n",
    "        return curr_prob, tb_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see it at work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HMM(alphabet, hidden_states, β=initial_probabilities, \n",
    "        trans_probs=transition_probabilities, emit_probs= emission_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIGGGGG\n"
     ]
    }
   ],
   "source": [
    "seq = \"ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\"\n",
    "print(model.viterbi(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIGGGGG\n"
     ]
    }
   ],
   "source": [
    "print('GIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIGGGGG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIIIIGGG\n"
     ]
    }
   ],
   "source": [
    "# Exact example from slides\n",
    "sequence = \"ACGCGATC\"\n",
    "print(model.viterbi(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\n",
      "GIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIIIIGGGGGGGGGGGGGGGGGGGGGGGGGGGGIIIIIIIIIIIGGGGG\n"
     ]
    }
   ],
   "source": [
    "# A slightly more complex example\n",
    "sequence = \"ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\"\n",
    "print(sequence)\n",
    "print(model.viterbi(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Extended example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = ('Ai', 'Ci', 'Gi', 'Ti', 'Ag', 'Cg', 'Gg', 'Tg')\n",
    "alphabet = 'ACGT'\n",
    "\n",
    "initial_probabilities = {\n",
    "    'Ai' : 0.125,\n",
    "    'Ci' : 0.125,\n",
    "    'Gi' : 0.125,\n",
    "    'Ti' : 0.125,\n",
    "    'Ag' : 0.125,\n",
    "    'Cg' : 0.125,\n",
    "    'Gg' : 0.125,\n",
    "    'Tg' : 0.125\n",
    "}\n",
    "\n",
    "transition_probabilities = {\n",
    "    'Ai': { 'Ai' : 0.2, 'Ci' : 0.36, 'Gi' : 0.2, 'Ti' : 0.2, 'Ag' : 0.01, 'Cg' : 0.01, 'Gg' : 0.01, 'Tg' : 0.01 },\n",
    "    'Ci': { 'Ai' : 0.1, 'Ci' : 0.1, 'Gi' : 0.66, 'Ti' : 0.1, 'Ag' : 0.01, 'Cg' : 0.01, 'Gg' : 0.01, 'Tg' : 0.01 },\n",
    "    'Gi': { 'Ai' : 0.1, 'Ci' : 0.39, 'Gi' : 0.1, 'Ti' : 0.1, 'Ag' : 0.1, 'Cg' : 0.01, 'Gg' : 0.1, 'Tg' : 0.1 },\n",
    "    'Ti': { 'Ai' : 0.2, 'Ci' : 0.36, 'Gi' : 0.2, 'Ti' : 0.2, 'Ag' : 0.01, 'Cg' : 0.01, 'Gg' : 0.01, 'Tg' : 0.01 },\n",
    "    'Ag': { 'Ai' : 0.01, 'Ci' : 0.1, 'Gi' : 0.01, 'Ti' : 0.01, 'Ag' : 0.2175, 'Cg' : 0.2175, 'Gg' : 0.2175, 'Tg' : 0.2175 },\n",
    "    'Cg': { 'Ai' : 0.01, 'Ci' : 0.1, 'Gi' : 0.01, 'Ti' : 0.01, 'Ag' : 0.2175, 'Cg' : 0.2175, 'Gg' : 0.2175, 'Tg' : 0.2175 },\n",
    "    'Gg': { 'Ai' : 0.01, 'Ci' : 0.1, 'Gi' : 0.01, 'Ti' : 0.01, 'Ag' : 0.2175, 'Cg' : 0.2175, 'Gg' : 0.2175, 'Tg' : 0.2175 },\n",
    "    'Tg': { 'Ai' : 0.01, 'Ci' : 0.1, 'Gi' : 0.01, 'Ti' : 0.01, 'Ag' : 0.2175, 'Cg' : 0.2175, 'Gg' : 0.2175, 'Tg' : 0.2175 }\n",
    "}\n",
    "\n",
    "emission_probabilities = {\n",
    "    'Ai': { 'A' : 1, 'C' : 0.001, 'G' : 0.001, 'T' : 0.001 },\n",
    "    'Ci': { 'A' : 0.001, 'C' : 1, 'G' : 0.001, 'T' : 0.001 },\n",
    "    'Gi': { 'A' : 0.001, 'C' : 0.001, 'G' : 1, 'T' : 0.001 },\n",
    "    'Ti': { 'A' : 0.001, 'C' : 0.001, 'G' : 0.001, 'T' : 1 },\n",
    "    'Ag': { 'A' : 1, 'C' : 0.001, 'G' : 0.001, 'T' : 0.001 },\n",
    "    'Cg': { 'A' : 0.001, 'C' : 1, 'G' : 0.001, 'T' : 0.001 },\n",
    "    'Gg': { 'A' : 0.001, 'C' : 0.001, 'G' : 1, 'T' : 0.001 },\n",
    "    'Tg': { 'A' : 0.001, 'C' :0.0010, 'G' : 0.001, 'T' : 1 }\n",
    "}\n",
    "\n",
    "model = HMM(alphabet, hidden_states, trans_probs=transition_probabilities, \n",
    "            emit_probs=emission_probabilities, β = initial_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\n",
      "IIIIIggggggggggggggggggggggggggIIIIIIIIIIIIIIgggggggggggggggggggggggggggggggggggggggggggg\n"
     ]
    }
   ],
   "source": [
    "sequence = \"ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\"\n",
    "\n",
    "print(sequence)\n",
    "\n",
    "result = model.viterbi(sequence)\n",
    "result = result.replace(\"A\", \"\")\n",
    "result = result.replace(\"C\", \"\")\n",
    "result = result.replace(\"G\", \"\")\n",
    "result = result.replace(\"T\", \"\")\n",
    "result = result.replace(\"i\", \"I\")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
